---
# **[é‡è¦]** RAGå¯¾å¿œå¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
title: "ãƒ©ãƒ³ãƒ–ãƒƒã‚¯: ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°"
description: "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ã—ãŸç”Ÿãƒ‡ãƒ¼ã‚¿ã®å“è³ªå•é¡Œï¼ˆæ¬ æå€¤ã€å¤–ã‚Œå€¤ã€å‹ä¸æ•´åˆã€é‡è¤‡ï¼‰ã‚’ä¿®æ­£ã—ã€åˆ†æã«é©ã—ãŸå½¢å¼ã«æ•´å½¢ã™ã‚‹ãŸã‚ã®æ¨™æº–ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ‰‹é †ã€‚Pandasæ´»ç”¨ã«ã‚ˆã‚‹ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œã‚¬ã‚¤ãƒ‰"

# åˆ†é¡ï¼ˆRAGãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ï¼‰
tags:
  - runbook
  - data-cleansing
  - data-quality
  - pandas
  - operations
category: runbook
domain: data-analysis
difficulty: intermediate

# é–¢é€£æ€§ï¼ˆã‚°ãƒ©ãƒ•æ§‹é€ ç”¨ï¼‰
related_docs:
  - data-quality-analysis-process.md
  - anomaly-detection-playbook.md
  - anti-patterns-data-analysis.md
prerequisites:
  - Python 3.8+ç’°å¢ƒ
  - PandasåŸºç¤çŸ¥è­˜

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
created_at: 2025-11-30
updated_at: 2025-11-30
version: "1.0"
author: data-engineering-team
---

# ãƒ©ãƒ³ãƒ–ãƒƒã‚¯: ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°

**ã‚µãƒ¼ãƒ“ã‚¹**: ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆETLï¼‰
**æ‰€è¦æ™‚é–“**: 30-90åˆ†ï¼ˆãƒ‡ãƒ¼ã‚¿è¦æ¨¡ã«ã‚ˆã‚Šå¤‰å‹•ï¼‰
**å®Ÿè¡Œé »åº¦**: dailyï¼ˆè‡ªå‹•ãƒãƒƒãƒï¼‰ã¾ãŸã¯ on-demandï¼ˆæ‰‹å‹•å®Ÿè¡Œï¼‰
**å¯¾è±¡èª­è€…**: ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆ

## ğŸ“‹ æ¦‚è¦

### ã‚¿ã‚¹ã‚¯ã®ç›®çš„

ã“ã®ãƒ©ãƒ³ãƒ–ãƒƒã‚¯ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ã—ãŸç”Ÿãƒ‡ãƒ¼ã‚¿ã®å“è³ªå•é¡Œï¼ˆ**æ¬ æå€¤ã€å¤–ã‚Œå€¤ã€å‹ä¸æ•´åˆã€é‡è¤‡ã€å½¢å¼ä¸çµ±ä¸€**ï¼‰ã‚’ä¿®æ­£ã—ã€åˆ†æã«é©ã—ãŸå½¢å¼ã«æ•´å½¢ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ‰‹é †ã‚’æä¾›ã—ã¾ã™ã€‚Pandasãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æ´»ç”¨ã—ãŸæ®µéšçš„ãªå‡¦ç†ã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’å‘ä¸Šã•ã›ã€ä¸‹æµã®åˆ†æãƒ»æ©Ÿæ¢°å­¦ç¿’ã‚¿ã‚¹ã‚¯ã®ç²¾åº¦ã‚’ç¢ºä¿ã—ã¾ã™ã€‚

### å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°

**âœ… å®Ÿè¡Œã™ã¹ãå ´åˆ**:
- ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æã§å“è³ªã‚¹ã‚³ã‚¢ < 80%ã¨åˆ¤å®šã•ã‚ŒãŸå ´åˆ
- æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹çµ±åˆå¾Œã®åˆå›ãƒ‡ãƒ¼ã‚¿æ•´å½¢æ™‚
- å®šæœŸãƒãƒƒãƒå®Ÿè¡Œï¼ˆæ—¥æ¬¡/é€±æ¬¡ï¼‰ã§ã®ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
- ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«å¤‰æ›´ã«ä¼´ã†ãƒ‡ãƒ¼ã‚¿æ•´å½¢ãƒ«ãƒ¼ãƒ«æ›´æ–°æ™‚

**âŒ å®Ÿè¡Œã™ã¹ãã§ãªã„å ´åˆ**:
- ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å´ã§å“è³ªä¿è¨¼ã•ã‚Œã¦ã„ã‚‹ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿
- æ—¢ã«ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆé‡è¤‡å®Ÿè¡Œã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ç ´æãƒªã‚¹ã‚¯ï¼‰
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ï¼ˆåˆ¥é€”ã‚¹ãƒˆãƒªãƒ¼ãƒ ç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½¿ç”¨ï¼‰

## âš™ï¸ å‰ææ¡ä»¶

### å¿…è¦ãªæ¨©é™

- [x] ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¸ã®èª­ã¿å–ã‚Šæ¨©é™ï¼ˆDBæ¥ç¶šã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ç­‰ï¼‰
- [x] å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®æ›¸ãè¾¼ã¿æ¨©é™
- [x] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™

### å¿…è¦ãªãƒ„ãƒ¼ãƒ«

| ãƒ„ãƒ¼ãƒ« | ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | ç¢ºèªæ–¹æ³• |
|--------|-----------|---------|
| Python | 3.8+ | `python --version` |
| pandas | 1.5+ | `python -c "import pandas; print(pandas.__version__)"` |
| numpy | 1.23+ | `python -c "import numpy; print(numpy.__version__)"` |
| scikit-learn | 1.0+ (optional) | `python -c "import sklearn; print(sklearn.__version__)"` |

### ç’°å¢ƒå¤‰æ•°

```bash
# å¿…è¦ãªç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
export DATA_SOURCE_PATH="/data/raw/sales_data.csv"
export OUTPUT_PATH="/data/cleaned/sales_data_cleaned.csv"
export BACKUP_DIR="/data/backup"
export LOG_DIR="/data/logs"
```

## ğŸ“ äº‹å‰ç¢ºèªãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

**âš ï¸ [é‡è¦]** å®Ÿè¡Œå‰ã«ä»¥ä¸‹ã‚’å¿…ãšç¢ºèªã—ã¦ãã ã•ã„ã€‚

- [x] **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆæ¸ˆã¿**: ç”Ÿãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒ`${BACKUP_DIR}`ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹
  ```bash
  ls -lh ${BACKUP_DIR}/sales_data_$(date +%Y%m%d).csv.bak
  ```
- [x] **ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª**: ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ < 80%ã€ãƒ‡ã‚£ã‚¹ã‚¯ç©ºãå®¹é‡ > 10GB
  ```bash
  free -h | awk '/Mem:/ {print $3/$2*100"%"}'
  df -h ${OUTPUT_PATH%/*} | awk 'NR==2 {print $4}'
  ```
- [x] **ä¾å­˜ã‚µãƒ¼ãƒ“ã‚¹ç¨¼åƒ**: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆDB/ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼‰ãŒæ­£å¸¸ç¨¼åƒä¸­
- [x] **å®Ÿè¡Œæ™‚é–“å¸¯ç¢ºèª**: ãƒ”ãƒ¼ã‚¯æ™‚é–“å¤–ï¼ˆæ·±å¤œ0-6æ™‚æ¨å¥¨ï¼‰ã¾ãŸã¯æ‰‹å‹•å®Ÿè¡Œã®æ‰¿èªå–å¾—æ¸ˆã¿
- [x] **å“è³ªãƒ«ãƒ¼ãƒ«ç¢ºèª**: ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`cleansing_rules.yaml`ï¼‰ãŒæœ€æ–°ç‰ˆ

### ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç¢ºèª

```bash
# ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç¢ºèªï¼ˆ10GBä»¥ä¸Šå¿…è¦ï¼‰
df -h ${OUTPUT_PATH%/*}

# ãƒ¡ãƒ¢ãƒªç¢ºèªï¼ˆ8GBä»¥ä¸Šæ¨å¥¨ï¼‰
free -h

# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
ls -lh ${DATA_SOURCE_PATH}
wc -l ${DATA_SOURCE_PATH}

# Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªç¢ºèª
python -c "import pandas, numpy; print('OK')"
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**:
```
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda1       100G   80G   15G  85% /data

              total        used        free
Mem:           16Gi       8.0Gi       6.0Gi

-rw-r--r-- 1 user group 500M Nov 30 10:00 /data/raw/sales_data.csv
100000 /data/raw/sales_data.csv

OK
```

## ğŸ”§ å®Ÿè¡Œæ‰‹é †

### ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åˆæœŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

**ç›®çš„**: ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€åŸºæœ¬çµ±è¨ˆé‡ã¨å“è³ªå•é¡Œã‚’æŠŠæ¡ã™ã‚‹

**å®Ÿè¡Œå†…å®¹**:
```python
import pandas as pd
import numpy as np
from datetime import datetime

# ãƒ­ã‚°è¨­å®š
import logging
logging.basicConfig(
    filename=f"{os.getenv('LOG_DIR')}/cleansing_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
    level=logging.INFO
)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
logging.info("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹")
df = pd.read_csv(os.getenv('DATA_SOURCE_PATH'))
logging.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ, {len(df.columns)}åˆ—")

# åˆæœŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
print("=== åˆæœŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ« ===")
print(f"è¡Œæ•°: {len(df)}")
print(f"ã‚«ãƒ©ãƒ æ•°: {len(df.columns)}")
print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
print(f"\næ¬ æå€¤ã‚µãƒãƒªãƒ¼:\n{df.isnull().sum()}")
print(f"\né‡è¤‡è¡Œæ•°: {df.duplicated().sum()}")
print(f"\nãƒ‡ãƒ¼ã‚¿å‹:\n{df.dtypes}")
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
```
=== åˆæœŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ« ===
è¡Œæ•°: 100000
ã‚«ãƒ©ãƒ æ•°: 10
ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: 7.63 MB

æ¬ æå€¤ã‚µãƒãƒªãƒ¼:
order_id           0
customer_id        0
product_id       150
order_date         0
amount          1200
status             5
email            800
postal_code     2000
dtype: int64

é‡è¤‡è¡Œæ•°: 35

ãƒ‡ãƒ¼ã‚¿å‹:
order_id        int64
customer_id     int64
product_id     object  # â† æœ¬æ¥ã¯int64ã§ã‚ã‚‹ã¹ã
order_date     object  # â† æœ¬æ¥ã¯datetimeã§ã‚ã‚‹ã¹ã
amount         float64
status         object
email          object
postal_code    object
dtype: object
```

**âš ï¸ ã‚¨ãƒ©ãƒ¼æ™‚ã®å¯¾å‡¦**:
- ã‚¨ãƒ©ãƒ¼: `FileNotFoundError: [Errno 2] No such file or directory`
  - åŸå› : ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãƒ‘ã‚¹ãŒé–“é•ã£ã¦ã„ã‚‹
  - å¯¾å‡¦: ç’°å¢ƒå¤‰æ•°`DATA_SOURCE_PATH`ã‚’ç¢ºèªã€ãƒ‘ã‚¹ã‚’ä¿®æ­£
- ã‚¨ãƒ©ãƒ¼: `MemoryError`
  - åŸå› : ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¦ãƒ¡ãƒ¢ãƒªä¸è¶³
  - å¯¾å‡¦: `chunksize`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§åˆ†å‰²èª­ã¿è¾¼ã¿ã€ã¾ãŸã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

---

### ã‚¹ãƒ†ãƒƒãƒ—2: æ¬ æå€¤å‡¦ç†

**ç›®çš„**: æ¬ æå€¤ã‚’é©åˆ‡ãªæ–¹æ³•ï¼ˆå‰Šé™¤ã€è£œå®Œã€ãƒ•ãƒ©ã‚°åŒ–ï¼‰ã§å‡¦ç†ã™ã‚‹

**å®Ÿè¡Œå†…å®¹**:
```python
logging.info("æ¬ æå€¤å‡¦ç†é–‹å§‹")

# 2-1. å¿…é ˆã‚«ãƒ©ãƒ ã®æ¬ æè¡Œå‰Šé™¤
required_cols = ['order_id', 'customer_id', 'order_date', 'amount']
df_before = len(df)
df = df.dropna(subset=required_cols)
logging.info(f"å¿…é ˆã‚«ãƒ©ãƒ æ¬ æå‰Šé™¤: {df_before - len(df)}è¡Œå‰Šé™¤")

# 2-2. product_idæ¬ æã‚’è£œå®Œï¼ˆå¾Œæ–¹åŸ‹ã‚ã€ã¾ãŸã¯"UNKNOWN"ï¼‰
df['product_id'] = df['product_id'].fillna('UNKNOWN')
logging.info("product_idæ¬ æã‚’'UNKNOWN'ã§è£œå®Œ")

# 2-3. emailæ¬ æã‚’ãƒ•ãƒ©ã‚°åŒ–ã—ã¦ä¿æŒ
df['email_missing'] = df['email'].isnull().astype(int)
df['email'] = df['email'].fillna('no-email@example.com')
logging.info("emailæ¬ æã‚’ãƒ•ãƒ©ã‚°åŒ–ã—ã€ãƒ€ãƒŸãƒ¼å€¤ã§è£œå®Œ")

# 2-4. postal_codeæ¬ æã‚’ä¸­å¤®å€¤ã§è£œå®Œï¼ˆæ•°å€¤ã®å ´åˆï¼‰
# ã¾ãŸã¯æœ€é »å€¤ã§è£œå®Œï¼ˆã‚«ãƒ†ã‚´ãƒªã®å ´åˆï¼‰
if df['postal_code'].dtype == 'object':
    mode_value = df['postal_code'].mode()[0] if not df['postal_code'].mode().empty else 'UNKNOWN'
    df['postal_code'] = df['postal_code'].fillna(mode_value)
    logging.info(f"postal_codeæ¬ æã‚’æœ€é »å€¤'{mode_value}'ã§è£œå®Œ")

# 2-5. statusæ¬ æã‚’"pending"ã§è£œå®Œï¼ˆãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«ï¼‰
df['status'] = df['status'].fillna('pending')
logging.info("statusæ¬ æã‚’'pending'ã§è£œå®Œ")

print(f"\næ¬ æå€¤å‡¦ç†å¾Œ:\n{df.isnull().sum()}")
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
```
æ¬ æå€¤å‡¦ç†å¾Œ:
order_id           0
customer_id        0
product_id         0
order_date         0
amount             0
status             0
email              0
postal_code        0
email_missing      0
dtype: int64
```

**âš ï¸ ã‚¨ãƒ©ãƒ¼æ™‚ã®å¯¾å‡¦**:
- è­¦å‘Š: `SettingWithCopyWarning`
  - åŸå› : DataFrameã®ãƒ“ãƒ¥ãƒ¼æ“ä½œã«ã‚ˆã‚‹è­¦å‘Š
  - å¯¾å‡¦: `df = df.copy()`ã§æ˜ç¤ºçš„ã«ã‚³ãƒ”ãƒ¼ä½œæˆ

---

### ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ã¨å½¢å¼çµ±ä¸€

**ç›®çš„**: å„ã‚«ãƒ©ãƒ ã‚’é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿å‹ã«å¤‰æ›ã—ã€å½¢å¼ã‚’çµ±ä¸€ã™ã‚‹

**å®Ÿè¡Œå†…å®¹**:
```python
logging.info("ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›é–‹å§‹")

# 3-1. order_date ã‚’ datetimeå‹ã«å¤‰æ›
df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')
invalid_dates = df['order_date'].isnull().sum()
if invalid_dates > 0:
    logging.warning(f"æ—¥ä»˜å¤‰æ›å¤±æ•—: {invalid_dates}è¡Œ")
    df = df.dropna(subset=['order_date'])  # ç„¡åŠ¹ãªæ—¥ä»˜è¡Œã‚’å‰Šé™¤

# 3-2. product_id ã‚’æ•´æ•°ã«å¤‰æ›ï¼ˆ"UNKNOWN"ã¯-1ã«ï¼‰
df['product_id'] = df['product_id'].replace('UNKNOWN', '-1')
df['product_id'] = pd.to_numeric(df['product_id'], errors='coerce').fillna(-1).astype(int)

# 3-3. amount ã‚’ float64ã«çµ±ä¸€ï¼ˆã‚«ãƒ³ãƒé™¤å»ç­‰ï¼‰
if df['amount'].dtype == 'object':
    df['amount'] = df['amount'].str.replace(',', '').astype(float)

# 3-4. email ã‚’å°æ–‡å­—ã«çµ±ä¸€
df['email'] = df['email'].str.lower().str.strip()

# 3-5. postal_code ã‚’çµ±ä¸€å½¢å¼ï¼ˆæ—¥æœ¬: 123-4567ï¼‰
import re
def format_postal_code(code):
    if pd.isnull(code):
        return code
    # æ•°å­—ã®ã¿æŠ½å‡º
    digits = re.sub(r'\D', '', str(code))
    if len(digits) == 7:
        return f"{digits[:3]}-{digits[3:]}"
    return code

df['postal_code'] = df['postal_code'].apply(format_postal_code)

# 3-6. status ã‚’ã‚«ãƒ†ã‚´ãƒªå‹ã«å¤‰æ›ï¼ˆãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰
df['status'] = df['status'].astype('category')

print(f"\nå‹å¤‰æ›å¾Œã®ãƒ‡ãƒ¼ã‚¿å‹:\n{df.dtypes}")
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
```
å‹å¤‰æ›å¾Œã®ãƒ‡ãƒ¼ã‚¿å‹:
order_id               int64
customer_id            int64
product_id             int64
order_date    datetime64[ns]
amount               float64
status              category
email                 object
postal_code           object
email_missing          int64
dtype: object
```

**âš ï¸ ã‚¨ãƒ©ãƒ¼æ™‚ã®å¯¾å‡¦**:
- ã‚¨ãƒ©ãƒ¼: `ValueError: unconverted data remains`
  - åŸå› : æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒè¤‡æ•°æ··åœ¨
  - å¯¾å‡¦: `pd.to_datetime(..., format='%Y-%m-%d', errors='coerce')`ã§æŸ”è»Ÿå¤‰æ›

---

### ã‚¹ãƒ†ãƒƒãƒ—4: å¤–ã‚Œå€¤æ¤œå‡ºã¨å‡¦ç†

**ç›®çš„**: çµ±è¨ˆçš„æ‰‹æ³•ã§å¤–ã‚Œå€¤ã‚’æ¤œå‡ºã—ã€ã‚­ãƒ£ãƒƒãƒ”ãƒ³ã‚°ã¾ãŸã¯å‰Šé™¤ã™ã‚‹

**å®Ÿè¡Œå†…å®¹**:
```python
logging.info("å¤–ã‚Œå€¤å‡¦ç†é–‹å§‹")

# 4-1. amount ã®å¤–ã‚Œå€¤æ¤œå‡ºï¼ˆIQRæ³•ï¼‰
Q1 = df['amount'].quantile(0.25)
Q3 = df['amount'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df[(df['amount'] < lower_bound) | (df['amount'] > upper_bound)]
logging.info(f"å¤–ã‚Œå€¤æ¤œå‡º: {len(outliers)}è¡Œ (ä¸‹é™{lower_bound:.2f}, ä¸Šé™{upper_bound:.2f})")

# 4-2. å¤–ã‚Œå€¤ã‚’ã‚­ãƒ£ãƒƒãƒ”ãƒ³ã‚°ï¼ˆå‰Šé™¤ã§ã¯ãªãä¸Šä¸‹é™ã«ä¸¸ã‚ã‚‹ï¼‰
df['amount'] = df['amount'].clip(lower=lower_bound, upper=upper_bound)

# 4-3. è² ã®é‡‘é¡ã‚’0ã«ä¿®æ­£ï¼ˆãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«ï¼‰
negative_amount = (df['amount'] < 0).sum()
if negative_amount > 0:
    logging.warning(f"è² ã®é‡‘é¡ã‚’æ¤œå‡º: {negative_amount}è¡Œ â†’ 0ã«ä¿®æ­£")
    df['amount'] = df['amount'].clip(lower=0)

# 4-4. æœªæ¥æ—¥ä»˜ã®å‰Šé™¤
future_dates = df[df['order_date'] > pd.Timestamp.now()]
if len(future_dates) > 0:
    logging.warning(f"æœªæ¥æ—¥ä»˜ã‚’æ¤œå‡º: {len(future_dates)}è¡Œ â†’ å‰Šé™¤")
    df = df[df['order_date'] <= pd.Timestamp.now()]

print(f"å¤–ã‚Œå€¤å‡¦ç†å¾Œã®è¡Œæ•°: {len(df)}")
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
```
å¤–ã‚Œå€¤å‡¦ç†å¾Œã®è¡Œæ•°: 99800
```

**âš ï¸ ã‚¨ãƒ©ãƒ¼æ™‚ã®å¯¾å‡¦**:
- è­¦å‘Š: å¤–ã‚Œå€¤ãŒ50%ä»¥ä¸Š
  - åŸå› : é–¾å€¤è¨­å®šãŒå³ã—ã™ãã‚‹ã€ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒãŒæ­ªã‚“ã§ã„ã‚‹
  - å¯¾å‡¦: IQRä¿‚æ•°ã‚’1.5â†’3.0ã«ç·©å’Œã€ã¾ãŸã¯å¯¾æ•°å¤‰æ›å¾Œã«æ¤œå‡º

---

### ã‚¹ãƒ†ãƒƒãƒ—5: é‡è¤‡å‰Šé™¤ã¨ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«æ¤œè¨¼

**ç›®çš„**: é‡è¤‡è¡Œã‚’å‰Šé™¤ã—ã€ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«ã«é•åã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ä¿®æ­£ã™ã‚‹

**å®Ÿè¡Œå†…å®¹**:
```python
logging.info("é‡è¤‡å‰Šé™¤ãƒ»ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«æ¤œè¨¼é–‹å§‹")

# 5-1. å®Œå…¨é‡è¤‡è¡Œã®å‰Šé™¤
dup_before = df.duplicated().sum()
df = df.drop_duplicates()
logging.info(f"å®Œå…¨é‡è¤‡å‰Šé™¤: {dup_before}è¡Œå‰Šé™¤")

# 5-2. ä¸»ã‚­ãƒ¼é‡è¤‡ã®å‰Šé™¤ï¼ˆorder_idã§ã‚½ãƒ¼ãƒˆã—ã€æœ€æ–°ã‚’ä¿æŒï¼‰
df = df.sort_values('order_date', ascending=False)
dup_key = df.duplicated(subset=['order_id']).sum()
df = df.drop_duplicates(subset=['order_id'], keep='first')
logging.info(f"ä¸»ã‚­ãƒ¼é‡è¤‡å‰Šé™¤: {dup_key}è¡Œå‰Šé™¤")

# 5-3. ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«æ¤œè¨¼: order_dateãŒéå»3å¹´ä»¥å†…
three_years_ago = pd.Timestamp.now() - pd.DateOffset(years=3)
old_data = df[df['order_date'] < three_years_ago]
if len(old_data) > 0:
    logging.warning(f"å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡º: {len(old_data)}è¡Œï¼ˆ3å¹´ä»¥ä¸Šå‰ï¼‰ â†’ å‰Šé™¤")
    df = df[df['order_date'] >= three_years_ago]

# 5-4. ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«æ¤œè¨¼: statusã®å€¤ãŒè¨±å¯ãƒªã‚¹ãƒˆå†…
allowed_status = ['pending', 'completed', 'cancelled', 'refunded']
invalid_status = df[~df['status'].isin(allowed_status)]
if len(invalid_status) > 0:
    logging.warning(f"ç„¡åŠ¹ãªstatus: {len(invalid_status)}è¡Œ â†’ 'pending'ã«ä¿®æ­£")
    df.loc[~df['status'].isin(allowed_status), 'status'] = 'pending'

print(f"é‡è¤‡å‰Šé™¤ãƒ»ãƒ«ãƒ¼ãƒ«æ¤œè¨¼å¾Œã®æœ€çµ‚è¡Œæ•°: {len(df)}")
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
```
é‡è¤‡å‰Šé™¤ãƒ»ãƒ«ãƒ¼ãƒ«æ¤œè¨¼å¾Œã®æœ€çµ‚è¡Œæ•°: 99750
```

**âš ï¸ ã‚¨ãƒ©ãƒ¼æ™‚ã®å¯¾å‡¦**:
- è­¦å‘Š: ä¸»ã‚­ãƒ¼é‡è¤‡ãŒå¤šæ•°ï¼ˆ>10%ï¼‰
  - åŸå› : ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®å“è³ªå•é¡Œ
  - å¯¾å‡¦: ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ¼ãƒŠãƒ¼ã«å ±å‘Šã€ä¸Šæµã§ã®é‡è¤‡é˜²æ­¢ç­–ã‚’æ¤œè¨

---

### ã‚¹ãƒ†ãƒƒãƒ—6: æœ€çµ‚æ¤œè¨¼ã¨å‡ºåŠ›

**ç›®çš„**: ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°çµæœã‚’æ¤œè¨¼ã—ã€å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã™ã‚‹

**å®Ÿè¡Œå†…å®¹**:
```python
logging.info("æœ€çµ‚æ¤œè¨¼ãƒ»å‡ºåŠ›é–‹å§‹")

# 6-1. æœ€çµ‚å“è³ªãƒã‚§ãƒƒã‚¯
quality_report = {
    'total_rows': len(df),
    'missing_values': df.isnull().sum().sum(),
    'duplicate_rows': df.duplicated().sum(),
    'negative_amount': (df['amount'] < 0).sum(),
    'future_dates': (df['order_date'] > pd.Timestamp.now()).sum(),
    'invalid_status': (~df['status'].isin(allowed_status)).sum()
}

logging.info(f"å“è³ªãƒ¬ãƒãƒ¼ãƒˆ: {quality_report}")
print("\n=== æœ€çµ‚å“è³ªãƒ¬ãƒãƒ¼ãƒˆ ===")
for key, value in quality_report.items():
    print(f"{key}: {value}")

# 6-2. å“è³ªåŸºæº–ãƒã‚§ãƒƒã‚¯
assert quality_report['missing_values'] == 0, "æ¬ æå€¤ãŒæ®‹ã£ã¦ã„ã‚‹"
assert quality_report['duplicate_rows'] == 0, "é‡è¤‡è¡ŒãŒæ®‹ã£ã¦ã„ã‚‹"
assert quality_report['negative_amount'] == 0, "è² ã®é‡‘é¡ãŒæ®‹ã£ã¦ã„ã‚‹"

# 6-3. å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
output_path = os.getenv('OUTPUT_PATH')
df.to_csv(output_path, index=False)
logging.info(f"å‡ºåŠ›å®Œäº†: {output_path}")

# 6-4. ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
summary = {
    'execution_time': datetime.now().isoformat(),
    'input_rows': 100000,  # åˆæœŸè¡Œæ•°
    'output_rows': len(df),
    'rows_removed': 100000 - len(df),
    'quality_score': 100 - (quality_report['missing_values'] + quality_report['duplicate_rows']) / len(df) * 100
}

import json
with open(f"{os.getenv('LOG_DIR')}/summary_{datetime.now().strftime('%Y%m%d')}.json", 'w') as f:
    json.dump(summary, f, indent=2)

print(f"\nâœ… ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å®Œäº†: {output_path}")
print(f"å‡¦ç†æ¸ˆã¿è¡Œæ•°: {len(df)} (å‰Šé™¤: {100000 - len(df)}è¡Œ)")
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
```
=== æœ€çµ‚å“è³ªãƒ¬ãƒãƒ¼ãƒˆ ===
total_rows: 99750
missing_values: 0
duplicate_rows: 0
negative_amount: 0
future_dates: 0
invalid_status: 0

âœ… ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å®Œäº†: /data/cleaned/sales_data_cleaned.csv
å‡¦ç†æ¸ˆã¿è¡Œæ•°: 99750 (å‰Šé™¤: 250è¡Œ)
```

## âœ… æ¤œè¨¼ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

**[é‡è¦]** å…¨ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†å¾Œã€ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

- [x] **å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ**: å‡ºåŠ›ãƒ‘ã‚¹ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€ã‚µã‚¤ã‚ºãŒå¦¥å½“
  ```bash
  ls -lh ${OUTPUT_PATH}
  ```
- [x] **ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ä¸€è‡´**: å…¥åŠ›è¡Œæ•° - å‰Šé™¤è¡Œæ•° = å‡ºåŠ›è¡Œæ•°
  ```bash
  wc -l ${OUTPUT_PATH}
  ```
- [x] **å“è³ªåŸºæº–é”æˆ**: æ¬ æå€¤0ä»¶ã€é‡è¤‡0ä»¶ã€å‹å¤‰æ›ã‚¨ãƒ©ãƒ¼0ä»¶
  ```python
  df_check = pd.read_csv(os.getenv('OUTPUT_PATH'))
  assert df_check.isnull().sum().sum() == 0
  assert df_check.duplicated(subset=['order_id']).sum() == 0
  ```
- [x] **ãƒ­ã‚°è¨˜éŒ²**: å®Ÿè¡Œãƒ­ã‚°ãŒä¿å­˜ã•ã‚Œã€ã‚¨ãƒ©ãƒ¼ãŒãªã„ã“ã¨ã‚’ç¢ºèª
  ```bash
  tail -n 20 ${LOG_DIR}/cleansing_*.log
  ```
- [x] **ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç¢ºèª**: å“è³ªã‚¹ã‚³ã‚¢ â‰¥ 95%
  ```bash
  cat ${LOG_DIR}/summary_$(date +%Y%m%d).json
  ```

### æ¤œè¨¼ã‚³ãƒãƒ³ãƒ‰

```bash
# è¡Œæ•°ç¢ºèª
echo "å…¥åŠ›è¡Œæ•°: $(wc -l < ${DATA_SOURCE_PATH})"
echo "å‡ºåŠ›è¡Œæ•°: $(wc -l < ${OUTPUT_PATH})"

# ã‚µãƒ³ãƒ—ãƒ«ç¢ºèªï¼ˆå…ˆé ­5è¡Œï¼‰
head -n 5 ${OUTPUT_PATH}

# æ¬ æå€¤ç¢ºèªï¼ˆPythonãƒ¯ãƒ³ãƒ©ã‚¤ãƒŠãƒ¼ï¼‰
python -c "
import pandas as pd
df = pd.read_csv('${OUTPUT_PATH}')
print('æ¬ æå€¤:', df.isnull().sum().sum())
print('é‡è¤‡:', df.duplicated().sum())
"
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
```
å…¥åŠ›è¡Œæ•°: 100000
å‡ºåŠ›è¡Œæ•°: 99750
order_id,customer_id,product_id,order_date,amount,status,email,postal_code,email_missing
1,1001,5001,2024-11-30,12500.00,completed,user@example.com,123-4567,0
...

æ¬ æå€¤: 0
é‡è¤‡: 0
```

## ğŸš¨ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

ã‚ˆãã‚ã‚‹å•é¡Œã¨å¯¾å‡¦æ³•ï¼š

| ç—‡çŠ¶ | åŸå›  | å¯¾å‡¦æ³• | å‚è€ƒ |
|------|------|--------|------|
| `MemoryError`ç™ºç”Ÿ | ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º > åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª | `chunksize=10000`ã§åˆ†å‰²èª­ã¿è¾¼ã¿ã€ã¾ãŸã¯Daskä½¿ç”¨ | [Pandaså…¬å¼: chunksize](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) |
| `ValueError: could not convert string to float` | æ•°å€¤ã‚«ãƒ©ãƒ ã«éæ•°å€¤æ–‡å­—ï¼ˆ$, ã‚«ãƒ³ãƒç­‰ï¼‰æ··å…¥ | `errors='coerce'`ã§å¼·åˆ¶å¤‰æ›ã€ã¾ãŸã¯æ­£è¦è¡¨ç¾ã§å‰å‡¦ç† | [Pandaså…¬å¼: to_numeric](https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html) |
| å‡¦ç†æ™‚é–“ãŒ2æ™‚é–“è¶… | ãƒ‡ãƒ¼ã‚¿é‡éå¤šã€ã¾ãŸã¯éåŠ¹ç‡ãªå‡¦ç† | ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ“ä½œã«å¤‰æ›´ã€`apply`ã®ä½¿ç”¨ã‚’å‰Šæ¸› | [Pandasæœ€é©åŒ–ã‚¬ã‚¤ãƒ‰](https://pandas.pydata.org/docs/user_guide/enhancingperf.html) |
| å“è³ªã‚¹ã‚³ã‚¢ < 80% | ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ãŒä¸ååˆ† | ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã§ãƒ«ãƒ¼ãƒ«è¦‹ç›´ã—ã€é–¾å€¤èª¿æ•´ | [ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æãƒ—ãƒ­ã‚»ã‚¹](data-quality-analysis-process.md) |

### è©³ç´°ãªãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

#### å•é¡Œ1: æ—¥ä»˜å¤‰æ›ã‚¨ãƒ©ãƒ¼ï¼ˆå¤šæ§˜ãªå½¢å¼æ··åœ¨ï¼‰

**ç—‡çŠ¶ã®è©³ç´°**:
`ValueError: time data '30/11/2024' doesn't match format '%Y-%m-%d'`

**è¨ºæ–­æ–¹æ³•**:
```python
# æ—¥ä»˜å½¢å¼ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ç¢ºèª
print(df['order_date'].value_counts().head(10))
```

**è§£æ±ºæ‰‹é †**:
1. è¤‡æ•°å½¢å¼ã‚’è¨±å®¹ã™ã‚‹å¤‰æ›
   ```python
   from dateutil import parser
   df['order_date'] = df['order_date'].apply(lambda x: parser.parse(x) if pd.notnull(x) else x)
   ```
2. ã¾ãŸã¯ã€`pd.to_datetime(..., infer_datetime_format=True, errors='coerce')`
3. å¤‰æ›å¤±æ•—è¡Œã‚’ç¢ºèªã—ã€æ‰‹å‹•ä¿®æ­£ã¾ãŸã¯å‰Šé™¤

#### å•é¡Œ2: ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ã‚ˆã‚‹å‡¦ç†ä¸­æ–­

**ç—‡çŠ¶ã®è©³ç´°**:
å‡¦ç†é€”ä¸­ã§`Killed`ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã€ãƒ—ãƒ­ã‚»ã‚¹ãŒçµ‚äº†

**è¨ºæ–­æ–¹æ³•**:
```bash
# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–
watch -n 1 free -h
```

**è§£æ±ºæ‰‹é †**:
1. ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºèª­ã¿è¾¼ã¿ã«å¤‰æ›´
   ```python
   chunks = []
   for chunk in pd.read_csv('data.csv', chunksize=10000):
       # å„ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
       chunk_cleaned = clean_chunk(chunk)
       chunks.append(chunk_cleaned)
   df = pd.concat(chunks, ignore_index=True)
   ```
2. ãƒ‡ãƒ¼ã‚¿å‹ã®æœ€é©åŒ–ï¼ˆ`category`, `int8`ç­‰ï¼‰
3. ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’æ—©æœŸã«å‰Šé™¤

#### å•é¡Œ3: å¤–ã‚Œå€¤å‰Šé™¤ã§90%ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿å–ªå¤±

**ç—‡çŠ¶ã®è©³ç´°**:
IQRæ³•ã§å¤–ã‚Œå€¤å‰Šé™¤å¾Œã€ãƒ‡ãƒ¼ã‚¿ãŒ1ä¸‡è¡Œ â†’ 100è¡Œã«æ¿€æ¸›

**è¨ºæ–­æ–¹æ³•**:
```python
# åˆ†å¸ƒã®å¯è¦–åŒ–
import matplotlib.pyplot as plt
df['amount'].hist(bins=50)
plt.savefig('distribution.png')

# åˆ†ä½æ•°ç¢ºèª
print(df['amount'].describe())
```

**è§£æ±ºæ‰‹é †**:
1. å¯¾æ•°å¤‰æ›ã§åˆ†å¸ƒã‚’æ­£è¦åŒ–
   ```python
   df['amount_log'] = np.log1p(df['amount'])
   # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§å¤–ã‚Œå€¤æ¤œå‡º
   ```
2. IQRä¿‚æ•°ã‚’ç·©å’Œï¼ˆ1.5 â†’ 3.0ï¼‰
3. ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã§å¦¥å½“ãªä¸Šä¸‹é™ã‚’æ‰‹å‹•è¨­å®š

è©³ç´°ã¯ [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰](../templates/troubleshooting-template.md) ã‚’å‚ç…§ã€‚

## â®ï¸ ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹é †

**âš ï¸ [é‡è¦]** å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã®å¾©æ—§æ‰‹é †

### ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ãŒå¿…è¦ãªçŠ¶æ³

- ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œã®å“è³ªã‚¹ã‚³ã‚¢ < 50%ï¼ˆå¤§é‡ã®ãƒ‡ãƒ¼ã‚¿å–ªå¤±ï¼‰
- ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«é•åã«ã‚ˆã‚Šä¸‹æµå‡¦ç†ãŒã‚¨ãƒ©ãƒ¼
- å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã¾ãŸã¯ç©ºãƒ•ã‚¡ã‚¤ãƒ«

### ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹é †

#### ã‚¹ãƒ†ãƒƒãƒ—1: å®Ÿè¡Œä¸­æ–­

```bash
# Pythonãƒ—ãƒ­ã‚»ã‚¹ã®åœæ­¢ï¼ˆCtrl+C ã¾ãŸã¯ killï¼‰
ps aux | grep python | grep cleansing
kill -SIGTERM <PID>
```

#### ã‚¹ãƒ†ãƒƒãƒ—2: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒ

```bash
# æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ç¢ºèª
ls -lht ${BACKUP_DIR}/*.csv.bak | head -n 1

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒ
cp ${BACKUP_DIR}/sales_data_$(date +%Y%m%d).csv.bak ${DATA_SOURCE_PATH}

# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤ï¼ˆä¸å®Œå…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
rm -f ${OUTPUT_PATH}
```

#### ã‚¹ãƒ†ãƒƒãƒ—3: çŠ¶æ…‹ç¢ºèª

```bash
# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ç¢ºèª
wc -l ${DATA_SOURCE_PATH}
head -n 5 ${DATA_SOURCE_PATH}

# ãƒ­ã‚°ã®ç¢ºèª
tail -n 50 ${LOG_DIR}/cleansing_*.log
```

**å¾©æ—§ç¢ºèª**:
- [x] ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒã•ã‚ŒãŸ
- [x] è¡Œæ•°ãŒå…ƒã®å€¤ï¼ˆä¾‹: 100000è¡Œï¼‰ã¨ä¸€è‡´ã™ã‚‹
- [x] ä¸å®Œå…¨ãªå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‰Šé™¤ã•ã‚ŒãŸ

## ğŸ“Š æˆæœç‰©ã¨ãƒ­ã‚°

### æˆæœç‰©

| ãƒ•ã‚¡ã‚¤ãƒ« | å ´æ‰€ | å½¢å¼ | ä¿æŒæœŸé–“ |
|---------|------|------|---------|
| ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ | `${OUTPUT_PATH}` | CSV | 30æ—¥ï¼ˆãã®å¾Œã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼‰ |
| å“è³ªãƒ¬ãƒãƒ¼ãƒˆ | `${LOG_DIR}/summary_*.json` | JSON | 90æ—¥ |
| ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— | `${BACKUP_DIR}/*_YYYYMMDD.csv.bak` | CSV | 7æ—¥ |

### ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«

- **å®Ÿè¡Œãƒ­ã‚°**: `${LOG_DIR}/cleansing_YYYYMMDD_HHMMSS.log`
- **ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°**: å®Ÿè¡Œãƒ­ã‚°å†…ã«WARNING/ERRORãƒ¬ãƒ™ãƒ«ã§è¨˜éŒ²
- **ç›£æŸ»ãƒ­ã‚°**: `${LOG_DIR}/audit_YYYYMMDD.log`ï¼ˆå®Ÿè¡Œè€…ã€æ™‚åˆ»ã€çµæœã‚’è¨˜éŒ²ï¼‰

### å®Ÿè¡Œè¨˜éŒ²

å®Ÿè¡Œå¾Œã€ä»¥ä¸‹ã‚’JIRA/Linearãƒã‚±ãƒƒãƒˆã«è¨˜éŒ²ï¼š

- **å®Ÿè¡Œæ—¥æ™‚**: 2025-11-30 10:00:00
- **å®Ÿè¡Œè€…**: data-engineer-name
- **å®Ÿè¡Œçµæœ**: æˆåŠŸ / å¤±æ•—
- **å‡¦ç†ä»¶æ•°**: å…¥åŠ›100000è¡Œ â†’ å‡ºåŠ›99750è¡Œï¼ˆå‰Šé™¤250è¡Œï¼‰
- **æ‰€è¦æ™‚é–“**: 45åˆ†
- **å“è³ªã‚¹ã‚³ã‚¢**: 98.5%
- **å‚™è€ƒ**: product_idæ¬ æ150ä»¶ã€emailæ¬ æ800ä»¶ã‚’è£œå®Œ

## ğŸ”— é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

### ãƒ—ãƒ­ã‚»ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- [ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æãƒ—ãƒ­ã‚»ã‚¹](data-quality-analysis-process.md) - ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‰ã®å“è³ªè©•ä¾¡

### ãƒ—ãƒ¬ã‚¤ãƒ–ãƒƒã‚¯
- [ç•°å¸¸æ¤œçŸ¥åˆ¤æ–­ãƒ—ãƒ¬ã‚¤ãƒ–ãƒƒã‚¯](anomaly-detection-playbook.md) - ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œã®ç•°å¸¸æ¤œçŸ¥

### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- [ãƒ‡ãƒ¼ã‚¿å“è³ªãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](../templates/troubleshooting-template.md) - å“è³ªå•é¡Œã®è¨ºæ–­ã¨å¯¾å‡¦

### å‚è€ƒè³‡æ–™
- [Pandaså…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://pandas.pydata.org/docs/)
- [ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ç¦å‰‡äº‹é …](anti-patterns-data-analysis.md) - ã‚«ãƒ†ã‚´ãƒª2å‚ç…§
- "Data Wrangling with Python" (O'Reilly, 2016)

## ğŸ“ˆ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

### ç›£è¦–é …ç›®

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | é–¾å€¤ | ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ |
|-----------|------|-------------|
| å‡¦ç†æ™‚é–“ | < 90åˆ† | > 2æ™‚é–“ã§è­¦å‘Š |
| ã‚¨ãƒ©ãƒ¼ç‡ | < 1% | > 5%ã§è­¦å‘Š |
| ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ç‡ | < 5% | > 10%ã§è­¦å‘Š |
| å“è³ªã‚¹ã‚³ã‚¢ | â‰¥ 95% | < 80%ã§è­¦å‘Š |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | < 80% | > 90%ã§è­¦å‘Š |

### æˆåŠŸåŸºæº–

- å‡¦ç†å®Œäº†æ™‚é–“ < 90åˆ†ï¼ˆ100ä¸‡è¡Œã®å ´åˆï¼‰
- ã‚¨ãƒ©ãƒ¼ç‡ < 1%ï¼ˆå¤‰æ›ã‚¨ãƒ©ãƒ¼ã€æ¤œè¨¼å¤±æ•—ç­‰ï¼‰
- ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ç‡ < 5%ï¼ˆéåº¦ãªå‰Šé™¤ã¯ä¸Šæµå•é¡Œã‚’ç¤ºå”†ï¼‰
- å“è³ªã‚¹ã‚³ã‚¢ â‰¥ 95%ï¼ˆæ¬ æãƒ»é‡è¤‡ãƒ»å‹ã‚¨ãƒ©ãƒ¼ã®ç·åˆã‚¹ã‚³ã‚¢ï¼‰

## ğŸ”„ æ”¹å–„å±¥æ­´

### ãƒãƒ¼ã‚¸ãƒ§ãƒ³å±¥æ­´

| ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | æ—¥ä»˜ | å¤‰æ›´å†…å®¹ | å¤‰æ›´è€… |
|-----------|------|---------|-------|
| 1.0 | 2025-11-30 | åˆç‰ˆä½œæˆã€‚6ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°æ‰‹é †ã€ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°3äº‹ä¾‹è¿½åŠ  | data-engineering-team |

### æ—¢çŸ¥ã®å•é¡Œ

- **å•é¡Œ1**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼ˆ1000ä¸‡è¡Œè¶…ï¼‰ã§ãƒ¡ãƒ¢ãƒªä¸è¶³ãŒç™ºç”Ÿ
  - ãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ©ã‚¦ãƒ³ãƒ‰: Daskãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¸ã®ç§»è¡Œã‚’æ¤œè¨ä¸­ï¼ˆæ¬¡ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§å¯¾å¿œäºˆå®šï¼‰
- **å•é¡Œ2**: è¤‡æ•°ã®æ—¥ä»˜å½¢å¼ãŒæ··åœ¨ã™ã‚‹å ´åˆã€å¤‰æ›ç²¾åº¦ãŒä½ä¸‹
  - ãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ©ã‚¦ãƒ³ãƒ‰: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å´ã§å½¢å¼çµ±ä¸€ã‚’ä¾é ¼ä¸­

---

**æœ€çµ‚æ›´æ–°**: 2025-11-30
**æ¬¡å›ãƒ¬ãƒ“ãƒ¥ãƒ¼äºˆå®š**: 2026-02-28ï¼ˆå››åŠæœŸãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰
**ãƒ¡ãƒ³ãƒ†ãƒŠãƒ¼**: data-engineering-team
